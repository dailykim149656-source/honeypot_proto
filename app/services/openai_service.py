from openai import AzureOpenAI, OpenAI
from app.config import (
    AZURE_OPENAI_ENDPOINT, 
    AZURE_OPENAI_API_KEY, 
    AZURE_OPENAI_API_VERSION,
    AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
    GOOGLE_API_KEY,
    GEMINI_MODEL
)
from app.services.prompts import DOC_PROMPT, CODE_PROMPT
import json
import traceback
import uuid

def get_openai_client():
    return AzureOpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_version="2024-02-15-preview",
        azure_endpoint=AZURE_OPENAI_ENDPOINT
    )

def get_google_client():
    """Google Gemini í´ë¼ì´ì–¸íŠ¸ (ì±„íŒ…/ë¶„ì„ìš©)"""
    return OpenAI(
        api_key=GOOGLE_API_KEY,
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
    )

def get_embedding(text: str) -> list:
    client = get_openai_client()
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-large"
    )
    return response.data[0].embedding

def analyze_text_for_search(text: str, file_name: str, file_type: str = "doc") -> list:
    """
    [ë³µêµ¬ë¨] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ LLM(Gemini)ì— ë³´ë‚´ êµ¬ì¡°í™”ëœ JSON(ì²­í¬ ë¦¬ìŠ¤íŠ¸)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    file_type: 'code' ë˜ëŠ” 'doc' (ê·¸ ì™¸ëŠ” docìœ¼ë¡œ ì²˜ë¦¬)
    """
    client = get_google_client()
    
    # 1. íŒŒì¼ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if file_type == "code":
        system_prompt = CODE_PROMPT
    else:
        system_prompt = DOC_PROMPT
        
    user_message = f"""
    [Input Document Info]
    FileName: {file_name}
    FileType: {file_type}
    
    [Input Text]
    {text[:50000]} 
    
    (Note: If text is truncated, process only what is provided. Do not hallucinate.)
    """
    # 50000ì ì œí•œ: Gemini Context WindowëŠ” í¬ì§€ë§Œ ì•ˆì „í•˜ê²Œ ì œí•œ

    try:
        print(f"ğŸ§  Processing with Gemini ({file_type})... Input length: {len(text[:50000])}", flush=True)
        
        # Gemini í˜¸ì¶œ
        response = client.chat.completions.create(
            model=GEMINI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.1, # ì •í˜• ë°ì´í„° ì¶”ì¶œì´ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
            response_format={"type": "json_object"},
            max_tokens=16000,
            timeout=120
        )
        
        print("âœ… Gemini response received.", flush=True)
        response_text = response.choices[0].message.content

        print("\n=== [Gemini Response Output] ===")
        print(response_text)
        print("================================\n")
        
        # JSON íŒŒì‹±
        try:
            parsed = json.loads(response_text)
            
            if isinstance(parsed, list):
                chunks = parsed
            elif isinstance(parsed, dict):
                # ìµœìƒìœ„ í‚¤ê°€ í•˜ë‚˜ê³  ê·¸ ê°’ì´ ë¦¬ìŠ¤íŠ¸ë¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
                # ex: {"chunks": [...]} or {"data": [...]}
                found_list = False
                for key, value in parsed.items():
                    if isinstance(value, list):
                        chunks = value
                        found_list = True
                        break
                if not found_list:
                    # ê·¸ëƒ¥ ë”•ì…”ë„ˆë¦¬ í•˜ë‚˜ë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ìŒˆ
                    chunks = [parsed]
            else:
                chunks = []
                
            # í•„ìˆ˜ í•„ë“œ ë³´ì •
            print(f"Generated {len(chunks)} chunks.")
            for chunk in chunks:
                if not chunk.get("id"):
                    chunk["id"] = f"{uuid.uuid4()}"
                if not chunk.get("fileName"):
                    chunk["fileName"] = file_name
                if not chunk.get("chunkMeta"):
                    chunk["chunkMeta"] = {}
                
            return chunks
            
        except json.JSONDecodeError:
            print(f"âŒ Gemini response is not valid JSON: {response_text[:100]}...")
            return []
            
    except Exception as e:
        print(f"âŒ Gemini Chat Completion failed: {e}")
        traceback.print_exc()
        return []
    
def analyze_files_for_handover(file_context: str) -> dict:
    """íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¸ìˆ˜ì¸ê³„ì„œ JSON ìƒì„± - í”„ë¡ íŠ¸ì—”ë“œ HandoverData í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    from app.services.search_service import get_search_client
    
    client = get_openai_client()
    
    # Azure Searchì—ì„œ ëª¨ë“  ë¬¸ì„œì˜ ì‹¤ì œ ë‚´ìš© ì§ì ‘ ê²€ìƒ‰
    print("ğŸ“„ Azure Searchì—ì„œ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    try:
        search_client = get_search_client()
        results = search_client.search(search_text="*", include_total_count=True, top=10)
        
        doc_contents = []
        for result in results:
            file_name = result.get("file_name", "Unknown")
            content = result.get("content", "")
            if content and len(content) > 0:
                # ìµœëŒ€ 1000ìê¹Œì§€ë§Œ í¬í•¨
                content_preview = content[:1000]
                doc_contents.append(f"[íŒŒì¼: {file_name}]\n{content_preview}\n")
                print(f"âœ… ë¬¸ì„œ í¬í•¨ë¨: {file_name} ({len(content)} ê¸€ì)")
        
        if doc_contents:
            print(f"ğŸ“‹ {len(doc_contents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
            indexed_context = "\n".join(doc_contents)
            file_context = indexed_context if not file_context else file_context + "\n\n---\n\n" + indexed_context
        else:
            print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
    except Exception as e:
        print(f"âš ï¸  ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
    
    # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë§¤ìš° ì§§ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
    if not file_context or len(file_context.strip()) < 20:
        print("â„¹ï¸  íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•¨ - ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€")
        file_context += """

[ìƒ˜í”Œ: í”„ë¡œì íŠ¸ í˜„í™© ë³´ê³ ]
í”„ë¡œì íŠ¸ëª…: ì‹œìŠ¤í…œ ê³ ë„í™”
ë‹´ë‹¹ì: ê¹€ì² ìˆ˜ ê³¼ì¥ (kim.cs@company.com)
ì¸ìˆ˜ì: ì´ì˜í¬ ëŒ€ë¦¬ (lee.yh@company.com)
ì¸ìˆ˜ ì˜ˆì •ì¼: 2025-02-15
ê°œë°œí˜„í™©: 70% ì§„í–‰ ì¤‘ (ë©”ì¸ ê¸°ëŠ¥ ê°œë°œ ì™„ë£Œ, ìµœì í™” ì§„í–‰ ì¤‘)
ì£¼ìš” ë‹´ë‹¹ ì—…ë¬´: ë°±ì—”ë“œ API ê°œë°œ, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„, ë³´ì•ˆ êµ¬í˜„
íŒ€ì›: ë°•ì¤€í˜¸(í”„ë¡ íŠ¸ì—”ë“œ), ìµœë¯¼ìˆ˜(QA)
ìœ„í—˜ìš”ì†Œ: ì¼ì • ì§€ì—° ê°€ëŠ¥ì„± (2ì£¼)
ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤: 2025-02-01 ì•ŒíŒŒ í…ŒìŠ¤íŠ¸"""
    
    print(f"ğŸ“Š ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(file_context)} ê¸€ì")

    system_message = """
ë‹¹ì‹ ì€ ì¸ìˆ˜ì¸ê³„ì„œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

ì•„ë˜ ìë£ŒëŠ” AI Search ì¸ë±ìŠ¤ì—ì„œ ì¶”ì¶œëœ ì—…ë¬´ ë¬¸ì„œì˜ ìš”ì•½ ë˜ëŠ” ì›ë¬¸ì…ë‹ˆë‹¤. ìë£Œê°€ ë§ì„ ê²½ìš° ì¤‘ë³µë˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ í†µí•©Â·ìš”ì•½í•˜ê³ , ì‹¤ì œ ì¸ìˆ˜ì¸ê³„ì„œì²˜ëŸ¼ êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ìë£Œì— í¬í•¨ëœ ì •ë³´ëŠ” ìµœëŒ€í•œ ë°˜ì˜í•˜ê³ , ìë£Œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ëŠ” í•­ëª©ì€ ë¹ˆ ë°°ì—´([]) ë˜ëŠ” ë¹ˆ ë¬¸ìì—´("")ë¡œ ì±„ì›Œì£¼ì„¸ìš”. ìë£Œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ìš”ì•½í•´ë„ ë©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ (í”„ë¡ íŠ¸ì—”ë“œ ìš”êµ¬ì‚¬í•­ì— ë§ì¶¤):
{
    "overview": {
        "transferor": {"name": "ì¸ê³„ìëª…", "position": "ì§ê¸‰/ë¶€ì„œ", "contact": "ì—°ë½ì²˜"},
        "transferee": {"name": "ì¸ìˆ˜ìëª…", "position": "ì§ê¸‰/ë¶€ì„œ", "contact": "ì—°ë½ì²˜", "startDate": "ì‹œì‘ì¼"},
        "reason": "ì¸ìˆ˜ì¸ê³„ ì‚¬ìœ ",
        "background": "ì—…ë¬´ ë°°ê²½",
        "period": "ê·¼ë¬´ ê¸°ê°„",
        "schedule": [{"date": "ë‚ ì§œ", "activity": "í™œë™"}]
    },
    "jobStatus": {
        "title": "ì§ì±…",
        "responsibilities": ["ì±…ì„ë‚´ìš©1", "ì±…ì„ë‚´ìš©2"],
        "authority": "ê¶Œí•œ",
        "reportingLine": "ë³´ê³ ì²´ê³„",
        "teamMission": "íŒ€ ë¯¸ì…˜",
        "teamGoals": ["ëª©í‘œ1", "ëª©í‘œ2"]
    },
    "priorities": [
        {"rank": 1, "title": "ìš°ì„ ê³¼ì œëª…", "status": "ìƒíƒœ", "solution": "í•´ê²°ë°©ì•ˆ", "deadline": "ë§ˆê°ì¼"}
    ],
    "stakeholders": {
        "manager": "ìƒê¸‰ì",
        "internal": [{"name": "ì´ë¦„", "role": "ì—­í• "}],
        "external": [{"name": "ì´ë¦„", "role": "ì—­í• "}]
    },
    "teamMembers": [
        {"name": "íŒ€ì›ëª…", "position": "ì§ê¸‰", "role": "ì—­í• ", "notes": "ë¹„ê³ "}
    ],
    "ongoingProjects": [
        {"name": "í”„ë¡œì íŠ¸ëª…", "owner": "ë‹´ë‹¹ì", "status": "ìƒíƒœ", "progress": 50, "deadline": "ë§ˆê°ì¼", "description": "ì„¤ëª…"}
    ],
    "risks": {"issues": "í˜„ì•ˆ", "risks": "ìœ„í—˜ìš”ì†Œ"},
    "roadmap": {"shortTerm": "ë‹¨ê¸°ê³„íš", "longTerm": "ì¥ê¸°ê³„íš"},
    "resources": {
        "docs": [{"category": "ë¶„ë¥˜", "name": "ë¬¸ì„œëª…", "location": "ìœ„ì¹˜"}],
        "systems": [{"name": "ì‹œìŠ¤í…œëª…", "usage": "ì‚¬ìš©ë°©ë²•", "contact": "ë‹´ë‹¹ì"}],
        "contacts": [{"category": "ë¶„ë¥˜", "name": "ì´ë¦„", "position": "ì§ê¸‰", "contact": "ì—°ë½ì²˜"}]
    },
    "checklist": [{"text": "í™•ì¸í•­ëª©", "completed": false}]
}
"""

    user_message = f"""
ì•„ë˜ëŠ” AI Search ì¸ë±ìŠ¤ì—ì„œ ì¶”ì¶œëœ ì—…ë¬´ ìë£Œ(ìš”ì•½/ì›ë¬¸)ì…ë‹ˆë‹¤. ì´ ìë£Œë“¤ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ ì—…ë¬´ ì¸ìˆ˜ì¸ê³„ì„œì²˜ëŸ¼ êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ìœ¼ë¡œ JSONì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.

ìë£Œê°€ ë§ìœ¼ë©´ ì¤‘ë³µ/ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ í†µí•©Â·ìš”ì•½í•˜ê³ , ìë£Œì— ìˆëŠ” ì •ë³´ëŠ” ìµœëŒ€í•œ ë°˜ì˜í•˜ì„¸ìš”. ì—†ëŠ” í•­ëª©ì€ ë¹ˆ ë°°ì—´([]) ë˜ëŠ” ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‚¨ê²¨ë‘ì„¸ìš”.

ìë£Œ:
{file_context}

ìœ„ì˜ JSON í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”.
"""

    try:
        print(f"ğŸš€ Azure OpenAI í˜¸ì¶œ ì‹œì‘...")
        print(f"   - ì—”ë“œí¬ì¸íŠ¸: {AZURE_OPENAI_ENDPOINT}")
        print(f"   - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(file_context)}")

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=4000,
            response_format={"type": "json_object"}
        )

        print(f"âœ… OpenAI ì‘ë‹µ ìˆ˜ì‹ ")
        response_text = response.choices[0].message.content
        print(f"   ì‘ë‹µ ê¸¸ì´: {len(response_text)} ê¸€ì")

        # JSON íŒŒì‹± ì‹œë„
        try:
            print(f"ğŸ” JSON íŒŒì‹± ì‹œë„...")
            result = json.loads(response_text)
            print(f"âœ… JSON íŒŒì‹± ì„±ê³µ - í‚¤: {list(result.keys())}")
            return result
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "overview": {
                    "transferor": {"name": "", "position": "", "contact": ""},
                    "transferee": {"name": "", "position": "", "contact": ""}
                },
                "jobStatus": {"title": "", "responsibilities": []},
                "priorities": [],
                "stakeholders": {"manager": "", "internal": [], "external": []},
                "teamMembers": [],
                "ongoingProjects": [],
                "risks": {"issues": "", "risks": ""},
                "roadmap": {"shortTerm": "", "longTerm": ""},
                "resources": {"docs": [], "systems": [], "contacts": []},
                "checklist": [],
                "rawContent": response_text
            }
    except Exception as e:
        print(f"âŒ Azure OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        # system_message ë“± ë¡œì»¬ ë³€ìˆ˜ ì°¸ì¡° ì—†ì´ ì—ëŸ¬ë§Œ ë°˜í™˜
        raise Exception(f"API ì—ëŸ¬: {e}")

def chat_with_context(query: str, context: str) -> str:
    client = get_openai_client()
    
    system_message = """ë‹¹ì‹ ì€ 'ê¿€ë‹¨ì§€' ì¸ìˆ˜ì¸ê³„ì„œ ìƒì„± AIì…ë‹ˆë‹¤. ğŸ¯

## í•µì‹¬ ì›ì¹™
1. **ë¬¸ì„œ ë‚´ìš©ì„ ë°˜ë“œì‹œ ë¨¼ì € ë¶„ì„**í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ì°¾ì€ **ì‹¤ì œ ì •ë³´**ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”
3. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ìœ ì—°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

## ì¸ìˆ˜ì¸ê³„ì„œ ìƒì„± ì‹œ ì°¸ê³ í•  êµ¬ì¡°
ì‚¬ìš©ìê°€ ì¸ìˆ˜ì¸ê³„ì„œ ìƒì„±ì„ ìš”ì²­í•˜ë©´, ì•„ë˜ ì„¹ì…˜ ì¤‘ ë¬¸ì„œì—ì„œ í™•ì¸ëœ ì •ë³´ë§Œ ì‘ì„±í•˜ì„¸ìš”:

1. **ì¸ì  ì •ë³´**: ì¸ê³„ì/ì¸ìˆ˜ì ì´ë¦„, ë¶€ì„œ, ì—°ë½ì²˜, ì¸ê³„ ì‚¬ìœ 
2. **ì§ë¬´ í˜„í™©**: ì§ë¬´ëª…, í•µì‹¬ ì±…ì„, ë³´ê³  ì²´ê³„
3. **ìš°ì„  ê³¼ì œ**: ì‹œê¸‰í•œ ê³¼ì œ Top 3, ì£¼ìš” ì´í•´ê´€ê³„ì, íŒ€ êµ¬ì„±ì›
4. **ì§„í–‰ ì¤‘ ì—…ë¬´**: í”„ë¡œì íŠ¸ í˜„í™©, ë¯¸ê²° ì‚¬í•­, í–¥í›„ ê³„íš
5. **í•µì‹¬ ìë£Œ**: ì°¸ê³  ë¬¸ì„œ, ì‹œìŠ¤í…œ ì ‘ê·¼ ì •ë³´, ì—°ë½ì²˜

## ë‹µë³€ ê·œì¹™
- ğŸ“Œ ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì€ **êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©**í•˜ì„¸ìš”
- ğŸ“Œ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ë§Œ "í•´ë‹¹ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í‘œì‹œ
- ğŸ“Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€
- ğŸ“Œ ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš” ğŸ"""

    user_message = f"""[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë¬¸ì„œì— ìˆëŠ” ì‹¤ì œ ì •ë³´ë¥¼ ì¸ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in chat_with_context: {e}")
        traceback.print_exc()
        raise